{"cells":[{"cell_type":"markdown","metadata":{"id":"l2DZdiKDPKuz"},"source":["# 9. 모델 프리징(Model Freezing)\n","\n","전이 학습 중 잘 학습 된 모델을 가져와 우리 연구에 사용할 수 있다. 데이터가 유사한 경우에는 추가적인 전체 학습 없이도 좋은 성능이 나올 수 있다. 따라서 피쳐 추출에 해당하는 합성곱 층의 변수를 업데이트 하지 않고 분류 파트에 해당하는 fully connected layer의 변수만 업데이트 할 수 있는데 이 때 변수가 업데이트 되지 않게 변수를 얼린다고 하여 이를 프리징(Freezing)이라고 한다.\n","\n","예를들어 고양이와 강아지를 분류하는 테스크를 진행한다.\n","이때, 사전 훈련된 모델의 경우 이미 고양이와 강아지의 피쳐에대해서 훈련이 되어 있기 때문에\n","CNN층의 변수는 업데이트하지 않고 마지막의 fc(분류기) 부분만 업데이트한다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fpraZs6QPKu4"},"outputs":[],"source":["import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader\n","\n","import torch.nn as nn\n","import torch.optim as optim"]},{"cell_type":"markdown","metadata":{"id":"50Gu5ICJPKu5"},"source":["## 9.1 GPU 연산 확인\n","빠른 병렬 처리를 위하여 CPU보다는 GPU연산이 사용된다. CUDA를 사용할 수 있는 NVIDIA 그래픽이 있다면 CUDA 버전과 torch 버전을 맞춰 본인의 컴퓨터를 세팅하도록 한다. 우리는 이런 수고스러움을 덜기 위해 무료로 GPU 연산을 제공하는 Google Colaboratory(이하 코랩)을 이용할 것이다. 코랩은 별도의 설치 없이 누구나 무료로 GPU를 사용할 수 있다."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xMHb45kHPKu5"},"outputs":[],"source":["# GPU vs CPU\n","# 현재 가능한 장치를 확인한다.\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"]},{"cell_type":"markdown","metadata":{"id":"0wRx_1Y3PKu6"},"source":["## 9.2 CIFAR10 데이터 불러오기"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q6ZXgljyPKu6"},"outputs":[],"source":["# 데이터 불러오기 및 전처리 작업\n","transform = transforms.Compose(\n","    [transforms.RandomCrop(32, padding=4),\n","     transforms.ToTensor(),\n","     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n","\n","test_transform = transforms.Compose(\n","    [transforms.ToTensor(),\n","     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n","\n","trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n","                                        download=True, transform=transform)\n","\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=16, shuffle=True) \n","\n","testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transform)\n","testloader = torch.utils.data.DataLoader(testset, batch_size=16,shuffle=False)\n","\n","# Class\n","#'plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'"]},{"cell_type":"markdown","metadata":{"id":"tPzKqg2ePKu6"},"source":["### 9.3 Pretrained model 불러오기\n","파이토치에서는 다양한 사전 학습 된 모델을 제공하고 있다.\n","https://pytorch.org/docs/stable/torchvision/models.html"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vbEc_91mPKu7"},"outputs":[],"source":["# AlexNet 불러오기 \n","# pretrained=True를 하면 AlexNet 구조와 사전 학습 된 파라메타를 모두 불러온다.\n","# pretrained=False를 하면 AlexNet 구조만 불러온다.\n","\n","model = torchvision.models.alexnet(pretrained=True)"]},{"cell_type":"code","source":["model"],"metadata":{"id":"6bVtAB4GTm_V"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I0OgsRulPKu7"},"outputs":[],"source":["# 모델의 구조를 보면 마지막 출력 노드가 1000개라는 것을 알 수 있다. 이미지넷 데이터를 활용해서 훈련했기 때문임\n","# 이는 1000개의 클래스를 가진 ImageNet 데이터를 이용하여 사전학습 된 모델이기 때문이다. \n","# 따라서 우리가 사용하는 CIFAR10 데이터에 맞게 출력층의 노드를 10개로 변경해야만 한다.\n","\n","num_ftrs = model.classifier[6].in_features # fc의 입력 노드 수를 산출한다. \n","model.classifier[6] = nn.Linear(num_ftrs, 10) # fc를 nn.Linear(num_ftrs, 10)로 대체한다.\n","model = model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HnbnCP7RPKu8"},"outputs":[],"source":["# 출력층의 노드가 10개로 바껴있다.\n","print(model)"]},{"cell_type":"markdown","metadata":{"id":"pYuT_E2iPKu8"},"source":["## 9.4 모델 프리징"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PV617SVIPKu8"},"outputs":[],"source":["# 파라메타 번호 확인 하기\n","i = 0\n","for name, param in model.named_parameters():\n","    \n","    print(i,name)\n","    i+= 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eVXalOTpPKu9"},"outputs":[],"source":["# 합성곱 층은 0~9까지이다. 따라서 9번째 변수까지 역추적을 비활성화 한 후 for문을 종료한다.\n","\n","for i, (name, param) in enumerate(model.named_parameters()):\n","    \n","    param.requires_grad = False\n","    if i == 9:\n","        print('end')\n","        break"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yHfeg5DrPKu9"},"outputs":[],"source":["# requires_grad 확인\n","print(model.features[0].weight.requires_grad)\n","print(model.features[0].bias.requires_grad)\n","print(model.features[3].weight.requires_grad)\n","print(model.features[3].bias.requires_grad)\n","print(model.features[6].weight.requires_grad)\n","print(model.features[6].bias.requires_grad)\n","print(model.features[8].weight.requires_grad)\n","print(model.features[8].bias.requires_grad)\n","print(model.features[10].weight.requires_grad)\n","print(model.features[10].bias.requires_grad)\n","print(model.classifier[1].weight.requires_grad)\n","print(model.classifier[1].bias.requires_grad)\n","print(model.classifier[4].weight.requires_grad)\n","print(model.classifier[4].bias.requires_grad)\n","print(model.classifier[6].weight.requires_grad)\n","print(model.classifier[6].bias.requires_grad)"]},{"cell_type":"markdown","metadata":{"id":"s5ugSm4HPKu9"},"source":["## 9.5 손실함수와 최적화 방법 정의"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S5-qUD77PKu-"},"outputs":[],"source":["criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-2)"]},{"cell_type":"markdown","metadata":{"id":"NBwexoRrPKu-"},"source":["## 9.6 프리징 된 사전학습 모델을 이용한 학습"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wn5Rz3VMPKu-"},"outputs":[],"source":["for epoch in range(20):\n","\n","    running_loss = 0.0\n","    for data in trainloader:\n","        \n","        inputs, labels = data[0].to(device), data[1].to(device)\n","          \n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","\n","    cost = running_loss / len(trainloader)        \n","    print('[%d] loss: %.3f' %(epoch + 1, cost))  \n","   \n","\n","print('Finished Training')"]},{"cell_type":"markdown","metadata":{"id":"4a41tdlaPKu_"},"source":["## 9.7 모델 평가"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N6BDIebgPKu_"},"outputs":[],"source":["correct = 0\n","total = 0\n","with torch.no_grad():\n","    model.eval()\n","    for data in testloader:\n","        images, labels = data\n","        outputs = model(images)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"11.Model Freezing.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":0}